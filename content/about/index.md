---
title: "About Me"
date: 2025-06-30T00:00:00-00:00
draft: false
---


{{< social-icons >}}

## What I am building?
**Parently - Trusted partner for parenting milestones**
- Parently is a free-to-use AI parenting assistant that feels just like chatting with a friend — someone who understands your parenting journey and guides you through each milestone. By building a personalized child profile, Parently predicts upcoming developmental stages and provides timely, tailored guidance that evolves with your family’s needs. Through natural conversations and continuous feedback, it creates a trusted loop of learning and support — helping parents feel more confident and prepared every step of the way. For more details, please see the [overview deck](https://drive.google.com/file/d/1YTE3ll5OE5KFRLvWD0c06U536ZHtrRJM/view?usp=drive_link) for more details.

## What I’m good at

- **Training & deploying AI/LLM systems** end-to-end (data → model → eval → rollout)
- **Large-scale data analysis** and experimentation frameworks
- **E-commerce × GenAI**: insights on real pain points and where GenAI *actually* helps
- **Turning research into product**: fast iteration loops, quality bars, and guardrails

## How I can help (and what I love to collaborate on)

- Mentorship on **LLM applications**, **scalable AI pipelines**, and **product validation**
- Sharing **e-commerce + GenAI** learnings (what works, what doesn’t)
- Collaborating on **early-stage business model & positioning** for AI products
- Thoughtful intros to **engineers/researchers** in my network when helpful

If you’re building human-centered AI or exploring adjacent spaces, I’d love to chat.

---

## Career Overview

**Applied Scientist at Amazon Search (Rufus)**  
*Mar. 2023 – Present*  
- Contributed to Amazon Rufus, an LLM-powered shopping assistant, by applying scientific insights to develop scalable systems for IFT training data augmentation and automating key evaluation processes, significantly enhancing model development, training data quality, and feature launches.

- Led the development of an innovative product insight feature on Amazon’s shopping website, enhancing the customer experience by simplifying product discovery. Implemented an LLM-powered system that generates concise product summaries, highlighting unique differentiators compared to competing products in search results.

**Applied Scientist at Amazon AWS AI Labs**  
*Jun. 2022 – Mar. 2023*  
- Worked in the Lex ASR Science Team, developing a chatbot tailored to customer needs. Focused on personalization techniques to enhance user experience rather than general ASR improvements. Key projects included improving ASR performance by integrating pre-trained language models for first-pass and second-pass rescoring.

**NLP Researcher at ByteDance AI Lab, Shanghai, China**  
*Dec. 2018 – Jul. 2021*  
- Worked in the Machine Translation Team on '[Volctrans](https://translate.volcengine.com/)', a multilingual machine translation service. Focused on research and development of multilingual translation techniques, as well as optimizing and deploying models for various translation directions. Demonstrated strong teamwork and collaboration skills.


---

## Research

[**Contrastive Learning for Many-to-many Multilingual Neural Machine Translation, ACL 2021 long**](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://aclanthology.org/2021.acl-long.21/&ved=2ahUKEwiapJmW0LaOAxXeMdAFHcqkLWMQFnoECA4QAQ&usg=AOvVaw0bdW31Aj5TUOZns31Gynmp)

_Authors: Xiao Pan, [Mingxuan Wang](https://mingxuan.github.io/), Liwei Wu, [Lei Li](https://lileicc.github.io/)_  
- Developed as the main contributor (90%+ contribution) an all-in-one multilingual machine translation model using contrastive learning. It supports 100 languages and achieves consistent improvements over the strong multilingual Transformer baseline. For non-English zero-shot directions, mRASP2 even achieves an improvement of average 10+ BLEU compared with the baseline (implemented using Fairseq, open-sourced at [https://github.com/PANXiao1994/mRASP2](https://github.com/PANXiao1994/mRASP2)).
- Trained and deployed a mRASP2 model using large-scale in-house corpora and deployed to the service "Volctrans".
- Handled the preprocessing process of large-scale corpora using shell script and distributed toolkits.

[**Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information, EMNLP 2020 long**](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://aclanthology.org/2020.emnlp-main.210/&ved=2ahUKEwiIsNmv0LaOAxXeJNAFHXbfIfsQFnoECCkQAQ&usg=AOvVaw1enorPlLOd0tP2fAQA3D4D)

_Authors: Zehui Lin*, Xiao Pan*, [Mingxuan Wang](https://mingxuan.github.io/), Xipeng Qiu, Jiangtao Feng, Hao Zhou, [Lei Li](https://lileicc.github.io/) (*: equal contribution)_  
- Developed a large-scale pre-training model for machine translation. First to propose "Exotic Full" scenario, where source and target languages are not in training corpora (implemented using Fairseq, open-sourced at [https://github.com/linzehui/mRASP](https://github.com/linzehui/mRASP)).
- Improved the MT performance of extremely low resource directions (<100k) by an average of 20+ BLEU; especially, for "Exotic Full" scenario, with only 12K parallel sentence pairs for Dutch—Portuguese, mRASP still reaches 10+ BLEU.
- Trained and deployed a pre-trained model using large-scale in-house corpora, improved production model performance and the training efficiency.

---

## Education

**Paris-Saclay University, Paris, France**  
*Master-2 in Applied Mathematics*  
Sep. 2017 – Oct. 2018

**Télécom ParisTech, Paris, France**  
*Master's Degree – Major in Data Science and Machine Learning*  
Sep. 2016 – Oct. 2018

**Tongji University, Shanghai, China**  
*Bachelor's Degree – Major in Communication Engineering*  
Sep. 2012 – Jul. 2016 